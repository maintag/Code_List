{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a26daaaf",
   "metadata": {},
   "source": [
    "# 공정 정보"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8bf46bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "\n",
    "# AMR 정보\n",
    "initial_energy = 15\n",
    "Residual_energy = 15\n",
    "\n",
    "#노드 정보\n",
    "start_node = 'SP'\n",
    "end_node = 'EP'\n",
    "process_nodes = ['A','B','C','D','CP']\n",
    "nodes = [start_node]+process_nodes+[end_node]\n",
    "\n",
    "#공정 및 이동 관련 정보\n",
    "process_times = {'A': 3, 'B': 4, 'C': 6, 'D': 4,'CP':0}\n",
    "\n",
    "# 소요시간 설정\n",
    "times = {\n",
    "        ('SP', 'A'): 2, ('SP', 'B'): 4, ('SP', 'C'): 8, ('SP', 'D'): 12, ('SP', 'CP'): 7,\n",
    "        ('A', 'A'): 0, ('A', 'B'): 3, ('A', 'C'): 7, ('A', 'D'): 10, ('A', 'CP'): 7, ('A', 'EP'): 11,\n",
    "        ('B', 'A'): 3, ('B', 'B'): 0, ('B', 'C'): 5, ('B', 'D'): 8, ('B', 'CP'): 7, ('B', 'EP'): 9,\n",
    "        ('C', 'A'): 7, ('C', 'B'): 5, ('C', 'C'): 0, ('C', 'D'): 4, ('C', 'CP'): 2, ('C', 'EP'): 6,\n",
    "        ('D', 'A'): 10, ('D', 'B'): 4, ('D', 'C'): 4, ('D', 'D'): 0, ('D', 'CP'): 8, ('D', 'EP'): 3,\n",
    "        ('CP', 'A'): 7, ('CP', 'B'): 7, ('CP', 'C'): 2, ('CP', 'D'): 8, ('CP', 'CP'): 0, ('CP', 'EP'): 7}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9783f0c7",
   "metadata": {},
   "source": [
    "# State, Action, Input encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf18f3ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 15]\n",
      "[0, 0, 1, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "class State:\n",
    "    def __init__(self, current_node, process_nodes, current_energy):\n",
    "        self.current_node = current_node\n",
    "        self.process_nodes = process_nodes\n",
    "        self.current_energy = current_energy\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"current_node: {self.current_node}, remaining_nodes: {self.process_nodes}, current_energy: {self.current_energy}\"\n",
    "\n",
    "class Action:\n",
    "    def __init__(self, next_node, process_nodes):\n",
    "        self.next_node = next_node\n",
    "        self.process_nodes = process_nodes  # 클래스 변수로 설정\n",
    "        \n",
    "    def get_next_node(self):  # 메서드 이름 변경\n",
    "        a = [int(node == self.next_node) for node in self.process_nodes]\n",
    "        return a\n",
    "\n",
    "# DQNetwork input encoding\n",
    "def encoding(nodes, state, process_nodes):\n",
    "    current_node_encoding = [int(node == state.current_node) for node in nodes]\n",
    "    process_nodes_encoding = [int(node in state.process_nodes) for node in process_nodes]\n",
    "    current_energy = [state.current_energy]\n",
    "    state_input = current_node_encoding + process_nodes_encoding + current_energy\n",
    "    return state_input\n",
    "\n",
    "state = State(start_node, process_nodes, initial_energy)\n",
    "\n",
    "\n",
    "state_size= encoding(nodes, state, process_nodes)\n",
    "num_actions = [int(node in state.process_nodes) for node in process_nodes]\n",
    "action_dim = Action('C', process_nodes)\n",
    "action_size = action_dim.get_next_node()  # 메서드 호출\n",
    "\n",
    "print(state_size)\n",
    "print(action_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d485a12",
   "metadata": {},
   "source": [
    "# Num to str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c410d9ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_dic= {0: 'A',1:'B',2: 'C',3: 'D',4:'CP'}\n",
    "action_num = int(np.random.choice(5))\n",
    "action = action_dic[action_num]\n",
    "action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a2e4880",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 11\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m action_dic[action_num]\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# 삭제한 후에 새로운 딕셔너리 생성\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m action_dic \u001b[38;5;241m=\u001b[39m {i: action_dic[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(action_dic))}\n\u001b[0;32m     12\u001b[0m action_dic\n",
      "Cell \u001b[1;32mIn[8], line 11\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m action_dic[action_num]\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# 삭제한 후에 새로운 딕셔너리 생성\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m action_dic \u001b[38;5;241m=\u001b[39m {i: action_dic[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(action_dic))}\n\u001b[0;32m     12\u001b[0m action_dic\n",
      "\u001b[1;31mKeyError\u001b[0m: 1"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "action_dic = {0: 'A', 1: 'B', 2: 'C', 3: 'D', 4: 'CP'}\n",
    "action_num = int(np.random.choice(5))\n",
    "action = action_dic[action_num]\n",
    "del action_dic[action_num]\n",
    "\n",
    "\n",
    "\n",
    "# 삭제한 후에 새로운 딕셔너리 생성\n",
    "action_dic = {i: action_dic[i] for i in range(len(action_dic))}\n",
    "action_dic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d0c632",
   "metadata": {},
   "source": [
    "# DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2a5a01ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# state = (차량 넘버, 현재 위치, 사용한 시간, 남은 공정, 남은 공정시간, 남은 에너지)\n",
    "# action = 다음 노드 선정\n",
    "# reward = 기다린 시간 + 사용한 시간\n",
    "# Epsilon_greedy\n",
    "\n",
    "\n",
    "class State():\n",
    "    def __init__(self, time):\n",
    "        self.car_num = car_num\n",
    "        self.current_node = current_node\n",
    "        self.residula_process = process_nodes\n",
    "        self.residual_process_time = current_time-time\n",
    "        self.current_energy = current_energy\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return f\"car_num: {self.car_num}, current_node: {self.current_node}, residula_process: {self.residula_process}, residual_process_time: {self.residual_process_time}, current_energy: {self.current_energy}\"\n",
    "        \n",
    "        \n",
    "# Q 네트워크를 정의하는 클래스\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, state):\n",
    "        super(QNetwork, self).__init__(state_size,action_size)\n",
    "        self.fc1 = nn.Linear(state_size, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, action_size)\n",
    "    \n",
    "    def forward(self, state):\n",
    "        x = torch.relu(self.fc1(state_size))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        q_values = self.fc3(x)\n",
    "        return q_values\n",
    "\n",
    "# DQN 에이전트 클래스\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size, learning_rate, gamma, epsilon, epsilon_decay):\n",
    "        self.q_network = QNetwork(state_size, action_size)\n",
    "        self.target_network = QNetwork(state_size, action_size)  # 타겟 네트워크\n",
    "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=learning_rate)\n",
    "        self.gamma = gamma  # 할인 계수\n",
    "        self.epsilon = epsilon  # 입실론 값 (탐험 vs. 이용)\n",
    "        self.epsilon_decay = epsilon_decay  # 입실론 감소율\n",
    "    \n",
    "    def choose_action(self):\n",
    "        if random.random() < self.epsilon:\n",
    "            action_num = str(np.random.choice(5))\n",
    "            action = action_dic[action_num]\n",
    "            while action is not process_nodes:\n",
    "                action_num = str(np.random.choice(5))\n",
    "                action = action_dic[action_num]\n",
    "            return action\n",
    "        else:\n",
    "            # 입실론 확률보다 크면 Q-value가 가장 높은 액션 선택\n",
    "            state = torch.Tensor(state)\n",
    "            q_values = self.q_network(state)\n",
    "            return torch.argmax(q_values).item()\n",
    "    \n",
    "    def learn(self, state, action, reward, next_state):\n",
    "        # Q-learning 업데이트 수행\n",
    "        state = torch.Tensor(state)\n",
    "        next_state = torch.Tensor(next_state)\n",
    "        q_values = self.q_network(state)\n",
    "        next_q_values = self.target_network(next_state)\n",
    "        max_next_q_value = torch.max(next_q_values)\n",
    "        target_q_value = reward + self.gamma * max_next_q_value\n",
    "        loss = nn.MSELoss()(q_values[action], target_q_value)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "    \n",
    "    def update_target_network(self):\n",
    "        # 타겟 네트워크 업데이트 (탐색 안정성을 위해)\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "    \n",
    "    def update_epsilon(self):\n",
    "        # 입실론 값을 감소시키는 함수\n",
    "        self.epsilon *= self.epsilon_decay\n",
    "        self.epsilon = max(0.01, self.epsilon)  # 최소 입실론 값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b2c0a337",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'state_size' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 47\u001b[0m\n\u001b[0;32m     44\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m model(state)\u001b[38;5;241m.\u001b[39mmax(\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Greedy action\u001b[39;00m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# Initialize DQN model, target model, and optimizer\u001b[39;00m\n\u001b[1;32m---> 47\u001b[0m input_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(state_size)\n\u001b[0;32m     48\u001b[0m output_size \u001b[38;5;241m=\u001b[39m num_actions\n\u001b[0;32m     49\u001b[0m dqn \u001b[38;5;241m=\u001b[39m DQN(input_size, output_size)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'state_size' is not defined"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "\n",
    "# Define DQN model\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Define Replay Buffer\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, transition):\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = transition\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "# Define epsilon-greedy policy\n",
    "def select_action(model, state, epsilon):\n",
    "    if random.random() < epsilon:\n",
    "        return random.randint(0, num_actions - 1)  # Random action\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            return model(state).max(1)[1].view(1, 1)  # Greedy action\n",
    "\n",
    "# Initialize DQN model, target model, and optimizer\n",
    "input_size = len(state_size)\n",
    "output_size = num_actions\n",
    "dqn = DQN(input_size, output_size)\n",
    "target_dqn = DQN(input_size, output_size)\n",
    "target_dqn.load_state_dict(dqn.state_dict())\n",
    "optimizer = optim.Adam(dqn.parameters())\n",
    "\n",
    "# Hyperparameters\n",
    "gamma = 0.99\n",
    "epsilon_start = 1.0\n",
    "epsilon_end = 0.01\n",
    "epsilon_decay = 0.995\n",
    "target_update = 10\n",
    "memory_capacity = 10000\n",
    "batch_size = 32\n",
    "\n",
    "# Initialize replay buffer\n",
    "replay_buffer = ReplayBuffer(memory_capacity)\n",
    "\n",
    "# Training loop\n",
    "epsilon = epsilon_start\n",
    "state = torch.tensor(state_size, dtype=torch.float32).unsqueeze(0)\n",
    "for episode in range(num_episodes):\n",
    "    episode_reward = 0\n",
    "    for t in range(max_steps):\n",
    "        # Select an action\n",
    "        action = select_action(dqn, state, epsilon)\n",
    "        next_node = action.next_node\n",
    "\n",
    "        # Update the environment and get the next state and reward\n",
    "        # Modify this part to match your specific environment and rewards\n",
    "        next_state, reward, done = update_environment(state, action)\n",
    "        next_state = torch.tensor(next_state, dtype=torch.float32).unsqueeze(0)\n",
    "        episode_reward += reward\n",
    "\n",
    "        # Store the transition in the replay buffer\n",
    "        replay_buffer.push((state, action, reward, next_state, done))\n",
    "\n",
    "        # Sample a random minibatch of transitions and perform DQN update\n",
    "        if len(replay_buffer.memory) > batch_size:\n",
    "            batch = replay_buffer.sample(batch_size)\n",
    "            batch = np.array(batch, dtype=object).T\n",
    "            state_batch = torch.cat(batch[0]).float()\n",
    "            action_batch = torch.cat(batch[1]).long()\n",
    "            reward_batch = torch.cat(batch[2]).float()\n",
    "            next_state_batch = torch.cat(batch[3]).float()\n",
    "            done_batch = torch.cat(batch[4]).int()\n",
    "\n",
    "            # Compute Q-values and target Q-values\n",
    "            Q = dqn(state_batch).gather(1, action_batch.unsqueeze(1))\n",
    "            max_next_Q = target_dqn(next_state_batch).max(1)[0].detach()\n",
    "            target_Q = reward_batch + gamma * max_next_Q * (1 - done_batch)\n",
    "\n",
    "            # Update the DQN\n",
    "            loss = nn.MSELoss()(Q, target_Q.unsqueeze(1))\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Update target DQN\n",
    "        if t % target_update == 0:\n",
    "            target_dqn.load_state_dict(dqn.state_dict())\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    # Decay epsilon\n",
    "    epsilon = max(epsilon_end, epsilon * epsilon_decay)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b314d48d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6777b7b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
