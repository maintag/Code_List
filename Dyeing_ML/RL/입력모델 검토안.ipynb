{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "414249e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import random\n",
    "import scipy.stats as stats\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingRegressor \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import plot_tree\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "#from imblearn.over_sampling import SMOTENC\n",
    "import xlsxwriter\n",
    "import openpyxl\n",
    "from openpyxl import load_workbook\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DataConversionWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "baea7a3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\leewa\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:494: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최적의 공정을 알고 싶은가요?네\n",
      "Dor_Red 값을 입력하세요 (비어있으면 0으로 처리): 1\n",
      "Dor_Blue 값을 입력하세요 (비어있으면 0으로 처리): \n",
      "Dor_BK 값을 입력하세요 (비어있으면 0으로 처리): \n",
      "Dor_DarkGrey 값을 입력하세요 (비어있으면 0으로 처리): \n",
      "Dor_Brown 값을 입력하세요 (비어있으면 0으로 처리): \n",
      "Dia_Yellow 값을 입력하세요 (비어있으면 0으로 처리): \n",
      "Dia_Red 값을 입력하세요 (비어있으면 0으로 처리): \n",
      "Dia_Blue 값을 입력하세요 (비어있으면 0으로 처리): \n",
      "Dia_Black 값을 입력하세요 (비어있으면 0으로 처리): \n",
      "Dia_Grey 값을 입력하세요 (비어있으면 0으로 처리): \n",
      "Dia_YellowBrown 값을 입력하세요 (비어있으면 0으로 처리): \n",
      "Dor_Yellow 값을 입력하세요 (비어있으면 0으로 처리): \n",
      "Dor_Black 값을 입력하세요 (비어있으면 0으로 처리): \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\leewa\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:43: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  result = getattr(asarray(obj), method)(*args, **kwds)\n",
      "C:\\Users\\leewa\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:43: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  result = getattr(asarray(obj), method)(*args, **kwds)\n",
      "C:\\Users\\leewa\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:43: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  result = getattr(asarray(obj), method)(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dye&Chemical: {'Dor_Red': 1.0, 'Dor_Blue': 0, 'Dor_BK': 0, 'Dor_DarkGrey': 0, 'Dor_Brown': 0, 'Dia_Yellow': 0, 'Dia_Red': 0, 'Dia_Blue': 0, 'Dia_Black': 0, 'Dia_Grey': 0, 'Dia_YellowBrown': 0, 'Dor_Yellow': 0, 'Dor_Black': 0, '배합_Sunsolt': 0.3, '배합_빙초산': 0.2}\n",
      "Process: 시작온도: 40, 상승속도1: 1.3, 상승온도1: 64, 상승온도1유지시간: 0, 상승속도2: 1.3, 상승온도2: 100, 상승온도2유지시간: 0, 상승속도3: 0.651, 상승온도3: 131, 상승온도유지시간3: 60, 하강속도1: 1.9722, 하강온도1: 64, 하강온도1유지시간: 0, 하강속도2: 0, 하강온도2: 64, 하강온도2유지시간: 0, 하강속도3: 0, 하강온도3: 64, 하강온도3유지시간: 0, 종료속도: 0, 종료온도: 64, 종료온도유지시간: 0\n",
      "Best KS: 0.11550739457954591\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\leewa\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:43: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  result = getattr(asarray(obj), method)(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# 모델 피클로 저장하는 함수\n",
    "def save_model(filename, model):\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "\n",
    "# 피클로 저장된 모델 불러오는 함수\n",
    "def load_model(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        model = pickle.load(f)\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "data = pd.read_csv('C:/Users/leewa/Desktop/이환/학습데이터.csv',encoding='euc-kr')\n",
    "x = data[['Dorosperse Red KKL', 'Dorosperse Blue KKL', 'Dorosperse B/K KKL',\n",
    "       'Dorosperse Dark Grey KKL', 'Dorosperse Brown K-3LR',\n",
    "       'Dianix Yellow AM-2R', 'Dianix Red AM-SLR', 'Dianix Blue AM-2G',\n",
    "       'Dianix Black AM-SLR', 'Dianix Grey AM-SLR', 'Dianix Yellow Brown AM-R',\n",
    "       'Dorosperse Yellow KKL', 'Dorosperse Black KKL','Dorosperse Red KKL_prop', 'Dorosperse Blue KKL_prop', 'Dorosperse B/K KKL_prop',\n",
    "       'Dorosperse Dark Grey KKL_prop', 'Dorosperse Brown K-3LR_prop',\n",
    "       'Dianix Yellow AM-2R_prop', 'Dianix Red AM-SLR_prop', 'Dianix Blue AM-2G_prop',\n",
    "       'Dianix Black AM-SLR_prop', 'Dianix Grey AM-SLR_prop', 'Dianix Yellow Brown AM-R_prop',\n",
    "       'Dorosperse Yellow KKL_prop', 'Dorosperse Black KKL_prop','배합_Sunsolt RM-340S', '배합_빙초산', 'Lab 염색 시작온도', 'Lab 염색 상승속도 #1',\n",
    "       'Lab 염색 상승온도 #1', 'Lab 염색 상승온도 #1 유지시간', 'Lab 염색 상승속도 #2',\n",
    "       'Lab 염색 상승온도 #2', 'Lab 염색 상승온도 #2 유지시간', 'Lab 염색 상승속도 #3',\n",
    "       'Lab 염색 상승온도 #3', 'Lab 염색 상승온도 #3 유지시간', 'Lab 염색 하강속도 #1',\n",
    "       'Lab 염색 하강온도 #1', 'Lab 염색 하강온도 #1 유지시간', 'Lab 염색 하강속도 #2',\n",
    "       'Lab 염색 하강온도 #2', 'Lab 염색 하강온도 #2 유지시간', 'Lab 염색 하강속도 #3',\n",
    "       'Lab 염색 하강온도 #3', 'Lab 염색 하강온도 #3 유지시간', 'Lab 염색 종료속도', 'Lab 염색 종료온도',\n",
    "       'Lab 염색 종료온도 유지시간']]\n",
    "y = data[['잔욕염색 검사_K/S']]\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y, train_size=0.8, test_size=0.2, random_state=42)\n",
    "estimator = GradientBoostingRegressor(n_estimators = 500,max_depth=8, min_samples_split=7,random_state=0)\n",
    "estimator.fit(x_train,y_train)\n",
    "y_pred = estimator.predict(x)\n",
    "\n",
    "\n",
    "class State:\n",
    "    def __init__(self):\n",
    "        self.시작온도 = 40        \n",
    "        self.상승속도1 = 0.5\n",
    "        self.상승온도1 = 64\n",
    "        self.상승온도1유지시간 = 0\n",
    "        self.상승속도2 = 0.5\n",
    "        self.상승온도2 = 100\n",
    "        self.상승온도2유지시간 = 0\n",
    "        self.상승속도3 = 0.5\n",
    "        self.상승온도3 = 120\n",
    "        self.상승온도유지시간3 = 10\n",
    "        self.하강속도1 = 1.9722\n",
    "        self.하강온도1 = 64\n",
    "        self.하강온도1유지시간 = 0\n",
    "        self.하강속도2 = 0\n",
    "        self.하강온도2 = 64\n",
    "        self.하강온도2유지시간 = 0\n",
    "        self.하강속도3 = 0\n",
    "        self.하강온도3 = 64\n",
    "        self.하강온도3유지시간 = 0\n",
    "        self.종료속도 = 0\n",
    "        self.종료온도 = 64\n",
    "        self.종료온도유지시간 = 0\n",
    "    \n",
    "    def __iter__(self):\n",
    "        yield self.상승속도1\n",
    "        yield self.상승속도2\n",
    "        yield self.상승속도3\n",
    "        yield self.상승온도3\n",
    "        yield self.상승온도유지시간3\n",
    "        \n",
    "        \n",
    "    def __repr__(self):\n",
    "        return f\"시작온도: {self.시작온도}, 상승속도1: {self.상승속도1}, 상승온도1: {self.상승온도1}, 상승온도1유지시간: {self.상승온도1유지시간}, 상승속도2: {self.상승속도2}, 상승온도2: {self.상승온도2}, 상승온도2유지시간: {self.상승온도2유지시간}, 상승속도3: {self.상승속도3}, 상승온도3: {self.상승온도3}, 상승온도유지시간3: {self.상승온도유지시간3}, 하강속도1: {self.하강속도1}, 하강온도1: {self.하강온도1}, 하강온도1유지시간: {self.하강온도1유지시간}, 하강속도2: {self.하강속도2}, 하강온도2: {self.하강온도2}, 하강온도2유지시간: {self.하강온도2유지시간}, 하강속도3: {self.하강속도3}, 하강온도3: {self.하강온도3}, 하강온도3유지시간: {self.하강온도3유지시간}, 종료속도: {self.종료속도}, 종료온도: {self.종료온도}, 종료온도유지시간: {self.종료온도유지시간}\"\n",
    "    \n",
    "    def get_ks(self):\n",
    "        statex = {\n",
    "            'Dorosperse Red KKL': [Dor_Red], \n",
    "            'Dorosperse Blue KKL': [Dor_Blue],\n",
    "            'Dorosperse B/K KKL': [Dor_BK], \n",
    "            'Dorosperse Dark Grey KKL': [Dor_DarkGrey],\n",
    "            'Dorosperse Brown K-3LR': [Dor_Brown], \n",
    "            'Dianix Yellow AM-2R': [Dia_Yellow],\n",
    "            'Dianix Red AM-SLR': [Dia_Red], \n",
    "            'Dianix Blue AM-2G': [Dia_Blue],\n",
    "            'Dianix Black AM-SLR': [Dia_Black],\n",
    "            'Dianix Grey AM-SLR': [Dia_Grey], \n",
    "            'Dianix Yellow Brown AM-R': [Dia_YellowBrown],\n",
    "            'Dorosperse Yellow KKL': [Dor_Yellow], \n",
    "            'Dorosperse Black KKL': [Dor_Black],\n",
    "            'Dorosperse Red KKL_prop': [Dor_Red/sum_values], \n",
    "            'Dorosperse Blue KKL_prop': [Dor_Blue/sum_values],\n",
    "            'Dorosperse B/K KKL_prop': [Dor_BK/sum_values], \n",
    "            'Dorosperse Dark Grey KKL_prop': [Dor_DarkGrey/sum_values],\n",
    "            'Dorosperse Brown K-3LR_prop': [Dor_Brown/sum_values], \n",
    "            'Dianix Yellow AM-2R_prop': [Dia_Yellow/sum_values],\n",
    "            'Dianix Red AM-SLR_prop': [Dia_Red/sum_values], \n",
    "            'Dianix Blue AM-2G_prop': [Dia_Blue/sum_values],\n",
    "            'Dianix Black AM-SLR_prop': [Dia_Black/sum_values],\n",
    "            'Dianix Grey AM-SLR_prop': [Dia_Grey/sum_values],\n",
    "            'Dianix Yellow Brown AM-R_prop': [Dia_YellowBrown/sum_values], \n",
    "            'Dorosperse Yellow KKL_prop': [Dor_Yellow/sum_values],\n",
    "            'Dorosperse Black KKL_prop': [Dor_Black/sum_values],\n",
    "            '배합_Sunsolt RM-340S': [배합_Sunsolt],\n",
    "            '배합_빙초산': [배합_빙초산], \n",
    "            'Lab 염색 시작온도': [self.시작온도], \n",
    "            'Lab 염색 상승속도 #1': [self.상승속도1],\n",
    "            'Lab 염색 상승온도 #1': [self.상승온도1],\n",
    "            'Lab 염색 상승온도 #1 유지시간': [self.상승온도1유지시간], \n",
    "            'Lab 염색 상승속도 #2': [self.상승속도2],\n",
    "            'Lab 염색 상승온도 #2': [self.상승온도2],\n",
    "            'Lab 염색 상승온도 #2 유지시간': [self.상승온도2유지시간],\n",
    "            'Lab 염색 상승속도 #3': [self.상승속도3],\n",
    "            'Lab 염색 상승온도 #3': [self.상승온도3],\n",
    "            'Lab 염색 상승온도 #3 유지시간': [self.상승온도유지시간3],\n",
    "            'Lab 염색 하강속도 #1': [self.하강속도1],\n",
    "            'Lab 염색 하강온도 #1': [self.하강온도1],\n",
    "            'Lab 염색 하강온도 #1 유지시간': [self.하강온도1유지시간],\n",
    "            'Lab 염색 하강속도 #2': [self.하강속도2],\n",
    "            'Lab 염색 하강온도 #2': [self.하강온도2],\n",
    "            'Lab 염색 하강온도 #2 유지시간': [self.하강온도2유지시간],\n",
    "            'Lab 염색 하강속도 #3': [self.하강속도3],\n",
    "            'Lab 염색 하강온도 #3': [self.하강온도3],\n",
    "            'Lab 염색 하강온도 #3 유지시간': [self.하강온도3유지시간],\n",
    "            'Lab 염색 종료속도': [self.종료속도],\n",
    "            'Lab 염색 종료온도': [self.종료온도],\n",
    "            'Lab 염색 종료온도 유지시간': [self.종료온도유지시간]}\n",
    "        statex = pd.DataFrame(statex)\n",
    "        state_ks = estimator.predict(statex)\n",
    "        statex = pd.DataFrame(statex)\n",
    "        return estimator.predict(statex)\n",
    "\n",
    "class Action:\n",
    "    def __init__(self):\n",
    "        self.상승속도1 = round(random.uniform(0.5, 1.5), 1)\n",
    "        self.상승속도2 = round(random.uniform(0.5, 1.5), 1)\n",
    "        self.상승속도3 = round(random.uniform(0.5, 1.7), 3)\n",
    "        self.상승온도3 = random.randint(120, 140)\n",
    "        self.상승온도유지시간3 = random.randint(1, 8) * 10\n",
    "    \n",
    "    def apply(self, state):\n",
    "        new_state = State()\n",
    "        new_state.상승속도1 = self.상승속도1\n",
    "        new_state.상승속도2 = self.상승속도2\n",
    "        new_state.상승속도3 = self.상승속도3\n",
    "        new_state.상승온도3 = self.상승온도3\n",
    "        new_state.상승온도유지시간3 = self.상승온도유지시간3\n",
    "        return new_state\n",
    "    \n",
    "class QLearning:\n",
    "    def __init__(self, alpha=0.5, gamma=1, epsilon=0.1):\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.q_table = {}\n",
    "        self.rewards_table = {}\n",
    "        \n",
    "    def get_q_value(self, state, action):\n",
    "        state_str = str(state).replace(\" \", \"\")\n",
    "        action_str = str(action).replace(\" \", \"\")\n",
    "        state_action = state_str + \"_\" + action_str\n",
    "        if state_action not in self.q_table:\n",
    "            self.q_table[state_action] = 0\n",
    "            self.rewards_table[state_action] = 0 # rewards_table 업데이트\n",
    "        return self.q_table[state_action]\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        if np.random.uniform() < self.epsilon:\n",
    "            return Action()\n",
    "        else:\n",
    "            actions = [Action() for _ in range(10)]\n",
    "            q_values = [self.get_q_value(state, action) for action in actions]\n",
    "            max_q_value = max(q_values)\n",
    "            if q_values.count(max_q_value) > 1:\n",
    "                best_actions = [a for a in actions if self.get_q_value(state, a) == max_q_value]\n",
    "                return random.choice(best_actions)\n",
    "            else:\n",
    "                return actions[np.argmax(q_values)]\n",
    "\n",
    "    def learn(self, state, action, reward, next_state):\n",
    "        td_target = reward + self.gamma * max([self.get_q_value(next_state, Action()) for _ in range(10)])\n",
    "        td_error = td_target - self.get_q_value(state, action)\n",
    "        state_action = str(state).replace(\" \", \"\") + \"_\" + str(action).replace(\" \", \"\")\n",
    "        self.q_table[state_action] += self.alpha * td_error\n",
    "        self.rewards_table[state_action] = reward # 리워드 값을 저장\n",
    "\n",
    "    def get_reward(self, state_ks, new_state_ks):\n",
    "        if new_state_ks < state_ks:\n",
    "            return 1-new_state_ks\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "\n",
    "    \n",
    "    def print_q_table(self):\n",
    "        print(self.q_table)\n",
    "        \n",
    "q_learning  = QLearning()\n",
    "\n",
    "class QValue:\n",
    "    def __init__(self, n_episodes, n_iterations, state_action_attrs):\n",
    "        self.n_episodes = n_episodes\n",
    "        self.n_iterations = n_iterations\n",
    "        self.state_action_attrs = state_action_attrs\n",
    "        self.best_state = None\n",
    "        self.best_ks_value = float('inf')\n",
    "        self.best_e_value = float('inf')\n",
    "        self.best_iteration = None\n",
    "        self.episode_data = []\n",
    "        self.column_names = ['Episode', '상승속도1', '상승속도2', '상승속도3', '상승온도3', '상승온도유지시간3', 'Best_KS_Value','Reward','Iteration', 'New_KS_state', 'KS_Value', 'e_Value']\n",
    "        \n",
    "        \n",
    "    def run(self, q_learning):\n",
    "        for episode in range(self.n_episodes):\n",
    "            state = State()\n",
    "            for iteration in range(self.n_iterations):\n",
    "                action = q_learning.choose_action(state)\n",
    "                new_state = action.apply(state)\n",
    "                reward = q_learning.get_reward(state.get_ks(),new_state.get_ks())\n",
    "                q_learning.learn(state, action, reward, new_state)\n",
    "                state = new_state\n",
    "\n",
    "                if new_state.get_ks() < self.best_ks_value :\n",
    "                    self.best_ks_value = new_state.get_ks()\n",
    "                    self.best_state = new_state\n",
    "                    self.best_iteration = iteration + 1\n",
    "                    self.best_reward = reward\n",
    "                \n",
    "\n",
    "            \n",
    "            episode_data = {\n",
    "                    'Episode': episode+1,\n",
    "                    '상승속도1': self.best_state.상승속도1,\n",
    "                    '상승속도2': self.best_state.상승속도2,\n",
    "                    '상승속도3': self.best_state.상승속도3,\n",
    "                    '상승온도3': self.best_state.상승온도3,\n",
    "                    '상승온도유지시간3': self.best_state.상승온도유지시간3,\n",
    "                    'Best_KS_Value': self.best_ks_value[0],\n",
    "                'Reward': self.best_reward,\n",
    "                'Iteration': iteration+1,\n",
    "                'New_KS_state': str(new_state), \n",
    "                'KS_Value': new_state.get_ks()}\n",
    "            self.episode_data.append(episode_data)\n",
    "\n",
    "\n",
    "            \n",
    "        return pd.DataFrame(self.episode_data, columns=self.column_names)\n",
    "    \n",
    "def get_user_input():\n",
    "    attrs = ['Dor_Red', 'Dor_Blue', 'Dor_BK', 'Dor_DarkGrey', 'Dor_Brown', 'Dia_Yellow', 'Dia_Red', 'Dia_Blue', 'Dia_Black', 'Dia_Grey', 'Dia_YellowBrown', 'Dor_Yellow', 'Dor_Black']\n",
    "    user_input_dict = {}\n",
    "\n",
    "    for attr in attrs:\n",
    "        user_input = input(f\"{attr} 값을 입력하세요 (비어있으면 0으로 처리): \")\n",
    "        if user_input == '':\n",
    "            user_input = 0\n",
    "        else:\n",
    "            user_input = float(user_input)\n",
    "        user_input_dict[attr] = user_input\n",
    "    global sum_values  # 전역 변수를 수정하려면 global 키워드를 사용해야 합니다.\n",
    "    sum_values = sum(list(user_input_dict.values())) \n",
    "    user_input_dict['배합_Sunsolt'] = 0.3\n",
    "    user_input_dict['배합_빙초산'] = 0.2\n",
    "    \n",
    "    return user_input_dict, sum_values  \n",
    "\n",
    "\n",
    "def main():\n",
    "    start_command = input(\"최적의 공정을 알고 싶은가요?\")\n",
    "    if start_command.strip().lower() == '네':\n",
    "        user_input_values, _ = get_user_input()  # 반환된 값을 변수에 할당\n",
    "\n",
    "        # 동적 변수 이름을 사용하여 값을 할당합니다.\n",
    "        for attr, value in user_input_values.items():\n",
    "            globals()[attr] = value\n",
    "\n",
    "        q_learning = QLearning(alpha=0.2, gamma=0.9, epsilon=0.5)\n",
    "        q_value = QValue(n_episodes=10, n_iterations=10, state_action_attrs=['상승속도1', '상승속도2', '상승속도3', '상승온도3', '상승온도유지시간3'])\n",
    "        q = q_value.run(q_learning)  # sum_values를 전달\n",
    "        print(\"Dye&Chemical:\", user_input_values)\n",
    "        print(\"Process:\", q_value.best_state)\n",
    "        print(\"Best KS:\", q_value.best_ks_value[0])\n",
    "        save_model(\"q_learning_model.pkl\", q_learning)\n",
    "\n",
    "# 메인 함수 실행\n",
    "main()\n",
    "\n",
    "# 저장한 피클 모델 불러오기\n",
    "loaded_q_learning = load_model(\"q_learning_model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47878369",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'new_state' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_28880\\1229378880.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mselected_action\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloaded_q_learning\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchoose_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Selected Action:\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mselected_action\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'new_state' is not defined"
     ]
    }
   ],
   "source": [
    "selected_action = loaded_q_learning.choose_action(new_state)\n",
    "print(\"Selected Action:\")\n",
    "print(selected_action.__dict__) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec22d03f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
