{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43a64f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "\n",
    "# 소요시간 설정\n",
    "times = {\n",
    "    ('SP', 'A'): 5, ('SP', 'B'): 8, ('SP', 'C'): 14, ('SP', 'D'): 16, ('SP', 'CP'): 7,\n",
    "    ('A', 'A'): 0, ('A', 'B'): 7, ('A', 'C'): 13, ('A', 'D'): 14, ('A', 'CP'): 7, ('A', 'EP'): 11,\n",
    "    ('B', 'A'): 6, ('B', 'B'): 0, ('B', 'C'): 11, ('B', 'D'): 12, ('B', 'CP'): 7, ('B', 'EP'): 9,\n",
    "    ('C', 'A'): 10, ('C', 'B'): 9, ('C', 'C'): 0, ('C', 'D'): 8, ('C', 'CP'): 2, ('C', 'EP'): 6,\n",
    "    ('D', 'A'): 13, ('D', 'B'): 8, ('D', 'C'): 10, ('D', 'D'): 0, ('D', 'CP'): 8, ('D', 'EP'): 3,\n",
    "    ('CP', 'A'): 10, ('CP', 'B'): 11, ('CP', 'C'): 8, ('CP', 'D'): 12, ('CP', 'CP'): 0, ('CP', 'EP'): 7}\n",
    "\n",
    "max_energy = 50\n",
    "process_nodes = ['A','B','C','D','CP']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc2bcb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "class State:\n",
    "    def __init__(self, node, process_nodes, current_energy):\n",
    "        self.node = node\n",
    "        self.process_nodes = process_nodes\n",
    "        self.current_energy = current_energy\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"current_node: {self.node}, remaining_nodes: {self.process_nodes}, current_energy: {self.current_energy}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e97b894e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearning:\n",
    "    def __init__(self, alpha, gamma, epsilon, decay):\n",
    "        self.alpha = alpha  # 학습률\n",
    "        self.gamma = gamma  # 할인 계수\n",
    "        self.epsilon = epsilon  # 탐험 비율\n",
    "        self.decay = decay\n",
    "        self.q_table = {}  # 상태-행동 값(Q-값)을 저장하는 테이블\n",
    "        self.rewards_table = {}  # 보상을 추적하는 테이블\n",
    "        \n",
    "    def param_update(self):\n",
    "        self.alpha = self.alpha\n",
    "        self.gamma = self.gamma*self.decay  # 할인 계수\n",
    "        self.epsilon = self.epsilon*self.decay  # 탐험 비율\n",
    "\n",
    "    def get_q_value(self, state, next_node):\n",
    "        # 만약 상태 또는 행동이 Q-테이블에 없으면 기본값 0을 반환\n",
    "        return self.q_table.get((state, next_node), 0.0)\n",
    "\n",
    "    def action(self,state):\n",
    "        if np.random.uniform() < self.epsilon:\n",
    "            return random.choice(state.process_nodes)\n",
    "        else:\n",
    "            # 가장 높은 Q-값을 가진 행동 선택\n",
    "            max_q_value = float('-inf')\n",
    "            best_action = None\n",
    "            best_next_node = None\n",
    "            for action in state.process_nodes:\n",
    "                q_value = self.get_q_value(state, action)\n",
    "                if q_value > max_q_value:\n",
    "                    max_q_value = q_value\n",
    "                    best_action = action\n",
    "            return best_action\n",
    "\n",
    "    def learn(self, state, next_node, reward, next_state):\n",
    "        if next_state.process_nodes:\n",
    "            # 공정 처리 노드가 남아 있을 때\n",
    "            max_q_value = max([self.get_q_value(next_state, next_action) for next_action in next_state.process_nodes])\n",
    "        else:\n",
    "            # 처리할 노드가 없을 때\n",
    "            max_q_value = 0.0\n",
    "        td_target = reward + self.gamma * max_q_value\n",
    "        td_error = td_target - self.get_q_value(state, next_node)\n",
    "        q_key = (state, next_node)\n",
    "        if q_key not in self.q_table:\n",
    "            self.q_table[q_key] = 0.0\n",
    "        self.q_table[q_key] += self.alpha * td_error\n",
    "        self.rewards_table[q_key] = reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93c632e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_node: EP, remaining_nodes: [], current_energy: 20\n",
      "current_node: EP, remaining_nodes: [], current_energy: 20\n",
      "current_node: EP, remaining_nodes: [], current_energy: 20\n",
      "current_node: EP, remaining_nodes: [], current_energy: 20\n",
      "current_node: EP, remaining_nodes: [], current_energy: 20\n",
      "current_node: EP, remaining_nodes: [], current_energy: 20\n",
      "current_node: EP, remaining_nodes: [], current_energy: 20\n",
      "current_node: EP, remaining_nodes: [], current_energy: 20\n",
      "current_node: EP, remaining_nodes: [], current_energy: 20\n",
      "current_node: EP, remaining_nodes: [], current_energy: 20\n"
     ]
    }
   ],
   "source": [
    "q_learning = QLearning(0.1,0.9,0.9,0.9)\n",
    "for episode in range(10):\n",
    "    node = 'SP'\n",
    "    process_nodes = ['A', 'B', 'C', 'D', 'CP']\n",
    "    current_energy = 20\n",
    "    total_spent_time = 0\n",
    "    \n",
    "    state = State(node, process_nodes, current_energy)\n",
    "    while state.process_nodes:\n",
    "        next_node = q_learning.action(state)\n",
    "        \n",
    "        process_nodes = state.process_nodes.copy()\n",
    "        process_nodes.remove(next_node)\n",
    "        \n",
    "        if next_node =='CP':\n",
    "            spent_time = times[(state.node,next_node)] + 28\n",
    "        else:\n",
    "            spent_time = times[(state.node,next_node)]\n",
    "        \n",
    "        total_spent_time += spent_time\n",
    "        reward =100-(spent_time + total_spent_time)\n",
    "            \n",
    "        next_state = State(next_node,process_nodes,current_energy)\n",
    "            \n",
    "        if next_state.current_energy < 0:\n",
    "            reward -= 100\n",
    "            \n",
    "        q_learning.learn(state,next_node,reward,next_state)\n",
    "        state = next_state\n",
    "        \n",
    "    next_node = 'EP'\n",
    "    spent_time = times[(state.node, next_node)]\n",
    "    total_spent_time +=spent_time\n",
    "    state.current_energy -= spent_time\n",
    "    \n",
    "    next_state = State(next_node,process_nodes,current_energy)\n",
    "    \n",
    "    reward =100-(spent_time + total_spent_time)\n",
    "    \n",
    "    if state.current_energy <10:\n",
    "        reward -= 100\n",
    "    \n",
    "    q_learning.learn(state,next_node,reward,next_state)\n",
    "    print(next_state)\n",
    "        \n",
    "        \n",
    "        \n",
    "    # Q-learning 에이전트로부터 다음 동작 선택"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2370d7d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'item'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m a \u001b[38;5;241m=\u001b[39m q_learning\u001b[38;5;241m.\u001b[39mq_table\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m      2\u001b[0m a\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'dict' object has no attribute 'item'"
     ]
    }
   ],
   "source": [
    "a = q_learning.q_table.item()\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9e75fffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "state = State('SP',process_nodes,20)\n",
    "action = 'A'\n",
    "\n",
    "print(Q_value(state,action,q_learning))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea302a51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f107453",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
