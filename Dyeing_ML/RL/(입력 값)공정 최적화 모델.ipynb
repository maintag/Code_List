{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "312fba88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import random\n",
    "import scipy.stats as stats\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingRegressor \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import plot_tree\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "#from imblearn.over_sampling import SMOTENC\n",
    "import xlsxwriter\n",
    "import openpyxl\n",
    "from openpyxl import load_workbook\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3afada43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\leewa\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:437: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('C:/Users/leewa/Desktop/공정변수.csv',encoding='euc-kr')\n",
    "x = data[['Dorosperse Red KKL', 'Dorosperse Blue KKL', 'Dorosperse B/K KKL',\n",
    "       'Dorosperse Dark Grey KKL', 'Dorosperse Brown K-3LR',\n",
    "       'Dianix Yellow AM-2R', 'Dianix Red AM-SLR', 'Dianix Blue AM-2G',\n",
    "       'Dianix Black AM-SLR', 'Dianix Grey AM-SLR', 'Dianix Yellow Brown AM-R',\n",
    "       'Dorosperse Yellow KKL', 'Dorosperse Black KKL','Dorosperse Red KKL_prop', 'Dorosperse Blue KKL_prop', 'Dorosperse B/K KKL_prop',\n",
    "       'Dorosperse Dark Grey KKL_prop', 'Dorosperse Brown K-3LR_prop',\n",
    "       'Dianix Yellow AM-2R_prop', 'Dianix Red AM-SLR_prop', 'Dianix Blue AM-2G_prop',\n",
    "       'Dianix Black AM-SLR_prop', 'Dianix Grey AM-SLR_prop', 'Dianix Yellow Brown AM-R_prop',\n",
    "       'Dorosperse Yellow KKL_prop', 'Dorosperse Black KKL_prop','배합_Sunsolt RM-340S', '배합_빙초산', 'Lab 염색 시작온도', 'Lab 염색 상승속도 #1',\n",
    "       'Lab 염색 상승온도 #1', 'Lab 염색 상승온도 #1 유지시간', 'Lab 염색 상승속도 #2',\n",
    "       'Lab 염색 상승온도 #2', 'Lab 염색 상승온도 #2 유지시간', 'Lab 염색 상승속도 #3',\n",
    "       'Lab 염색 상승온도 #3', 'Lab 염색 상승온도 #3 유지시간', 'Lab 염색 하강속도 #1',\n",
    "       'Lab 염색 하강온도 #1', 'Lab 염색 하강온도 #1 유지시간', 'Lab 염색 하강속도 #2',\n",
    "       'Lab 염색 하강온도 #2', 'Lab 염색 하강온도 #2 유지시간', 'Lab 염색 하강속도 #3',\n",
    "       'Lab 염색 하강온도 #3', 'Lab 염색 하강온도 #3 유지시간', 'Lab 염색 종료속도', 'Lab 염색 종료온도',\n",
    "       'Lab 염색 종료온도 유지시간']]\n",
    "y = data[['잔욕염색 검사_K/S']]\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y, train_size=0.8, test_size=0.2, random_state=42)\n",
    "estimator = GradientBoostingRegressor(n_estimators = 500,max_depth=8, min_samples_split=7,random_state=0)\n",
    "estimator.fit(x_train,y_train)\n",
    "y_pred = estimator.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64a8ea4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class State:\n",
    "    def __init__(self):\n",
    "        self.시작온도 = 40        \n",
    "        self.상승속도1 = 0.5\n",
    "        self.상승온도1 = 64\n",
    "        self.상승온도1유지시간 = 0\n",
    "        self.상승속도2 = 0.5\n",
    "        self.상승온도2 = 100\n",
    "        self.상승온도2유지시간 = 0\n",
    "        self.상승속도3 = 0.5\n",
    "        self.상승온도3 = 120\n",
    "        self.상승온도유지시간3 = 10\n",
    "        self.하강속도1 = 1.9722\n",
    "        self.하강온도1 = 64\n",
    "        self.하강온도1유지시간 = 0\n",
    "        self.하강속도2 = 0\n",
    "        self.하강온도2 = 64\n",
    "        self.하강온도2유지시간 = 0\n",
    "        self.하강속도3 = 0\n",
    "        self.하강온도3 = 64\n",
    "        self.하강온도3유지시간 = 0\n",
    "        self.종료속도 = 0\n",
    "        self.종료온도 = 64\n",
    "        self.종료온도유지시간 = 0\n",
    "    \n",
    "    def __iter__(self):\n",
    "        yield self.상승속도1\n",
    "        yield self.상승속도2\n",
    "        yield self.상승속도3\n",
    "        yield self.상승온도3\n",
    "        yield self.상승온도유지시간3\n",
    "        \n",
    "        \n",
    "    def __repr__(self):\n",
    "        return f\"시작온도: {self.시작온도}, 상승속도1: {self.상승속도1}, 상승온도1: {self.상승온도1}, 상승온도1유지시간: {self.상승온도1유지시간}, 상승속도2: {self.상승속도2}, 상승온도2: {self.상승온도2}, 상승온도2유지시간: {self.상승온도2유지시간}, 상승속도3: {self.상승속도3}, 상승온도3: {self.상승온도3}, 상승온도유지시간3: {self.상승온도유지시간3}, 하강속도1: {self.하강속도1}, 하강온도1: {self.하강온도1}, 하강온도1유지시간: {self.하강온도1유지시간}, 하강속도2: {self.하강속도2}, 하강온도2: {self.하강온도2}, 하강온도2유지시간: {self.하강온도2유지시간}, 하강속도3: {self.하강속도3}, 하강온도3: {self.하강온도3}, 하강온도3유지시간: {self.하강온도3유지시간}, 종료속도: {self.종료속도}, 종료온도: {self.종료온도}, 종료온도유지시간: {self.종료온도유지시간}\"\n",
    "    \n",
    "    def get_ks(self):\n",
    "        statex = {\n",
    "            'Dorosperse Red KKL': [Dor_Red], \n",
    "            'Dorosperse Blue KKL': [Dor_Blue],\n",
    "            'Dorosperse B/K KKL': [Dor_BK], \n",
    "            'Dorosperse Dark Grey KKL': [Dor_DarkGrey],\n",
    "            'Dorosperse Brown K-3LR': [Dor_Brown], \n",
    "            'Dianix Yellow AM-2R': [Dia_Yellow],\n",
    "            'Dianix Red AM-SLR': [Dia_Red], \n",
    "            'Dianix Blue AM-2G': [Dia_Blue],\n",
    "            'Dianix Black AM-SLR': [Dia_Black],\n",
    "            'Dianix Grey AM-SLR': [Dia_Grey], \n",
    "            'Dianix Yellow Brown AM-R': [Dia_YellowBrown],\n",
    "            'Dorosperse Yellow KKL': [Dor_Yellow], \n",
    "            'Dorosperse Black KKL': [Dor_Black],\n",
    "            'Dorosperse Red KKL_prop': [Dor_Red/sum_values], \n",
    "            'Dorosperse Blue KKL_prop': [Dor_Blue/sum_values],\n",
    "            'Dorosperse B/K KKL_prop': [Dor_BK/sum_values], \n",
    "            'Dorosperse Dark Grey KKL_prop': [Dor_DarkGrey/sum_values],\n",
    "            'Dorosperse Brown K-3LR_prop': [Dor_Brown/sum_values], \n",
    "            'Dianix Yellow AM-2R_prop': [Dia_Yellow/sum_values],\n",
    "            'Dianix Red AM-SLR_prop': [Dia_Red/sum_values], \n",
    "            'Dianix Blue AM-2G_prop': [Dia_Blue/sum_values],\n",
    "            'Dianix Black AM-SLR_prop': [Dia_Black/sum_values],\n",
    "            'Dianix Grey AM-SLR_prop': [Dia_Grey/sum_values],\n",
    "            'Dianix Yellow Brown AM-R_prop': [Dia_YellowBrown/sum_values], \n",
    "            'Dorosperse Yellow KKL_prop': [Dor_Yellow/sum_values],\n",
    "            'Dorosperse Black KKL_prop': [Dor_Black/sum_values],\n",
    "            '배합_Sunsolt RM-340S': [배합_Sunsolt],\n",
    "            '배합_빙초산': [배합_빙초산], \n",
    "            'Lab 염색 시작온도': [self.시작온도], \n",
    "            'Lab 염색 상승속도 #1': [self.상승속도1],\n",
    "            'Lab 염색 상승온도 #1': [self.상승온도1],\n",
    "            'Lab 염색 상승온도 #1 유지시간': [self.상승온도1유지시간], \n",
    "            'Lab 염색 상승속도 #2': [self.상승속도2],\n",
    "            'Lab 염색 상승온도 #2': [self.상승온도2],\n",
    "            'Lab 염색 상승온도 #2 유지시간': [self.상승온도2유지시간],\n",
    "            'Lab 염색 상승속도 #3': [self.상승속도3],\n",
    "            'Lab 염색 상승온도 #3': [self.상승온도3],\n",
    "            'Lab 염색 상승온도 #3 유지시간': [self.상승온도유지시간3],\n",
    "            'Lab 염색 하강속도 #1': [self.하강속도1],\n",
    "            'Lab 염색 하강온도 #1': [self.하강온도1],\n",
    "            'Lab 염색 하강온도 #1 유지시간': [self.하강온도1유지시간],\n",
    "            'Lab 염색 하강속도 #2': [self.하강속도2],\n",
    "            'Lab 염색 하강온도 #2': [self.하강온도2],\n",
    "            'Lab 염색 하강온도 #2 유지시간': [self.하강온도2유지시간],\n",
    "            'Lab 염색 하강속도 #3': [self.하강속도3],\n",
    "            'Lab 염색 하강온도 #3': [self.하강온도3],\n",
    "            'Lab 염색 하강온도 #3 유지시간': [self.하강온도3유지시간],\n",
    "            'Lab 염색 종료속도': [self.종료속도],\n",
    "            'Lab 염색 종료온도': [self.종료온도],\n",
    "            'Lab 염색 종료온도 유지시간': [self.종료온도유지시간]}\n",
    "        statex = pd.DataFrame(statex)\n",
    "        state_ks = estimator.predict(statex)\n",
    "        statex = pd.DataFrame(statex)\n",
    "        return estimator.predict(statex)\n",
    "\n",
    "class Action:\n",
    "    def __init__(self):\n",
    "        self.상승속도1 = round(random.uniform(0.5, 1.5), 1)\n",
    "        self.상승속도2 = round(random.uniform(0.5, 1.5), 1)\n",
    "        self.상승속도3 = round(random.uniform(0.5, 1.7), 3)\n",
    "        self.상승온도3 = random.randint(120, 140)\n",
    "        self.상승온도유지시간3 = random.randint(1, 8) * 10\n",
    "    \n",
    "    def apply(self, state):\n",
    "        new_state = State()\n",
    "        new_state.상승속도1 = self.상승속도1\n",
    "        new_state.상승속도2 = self.상승속도2\n",
    "        new_state.상승속도3 = self.상승속도3\n",
    "        new_state.상승온도3 = self.상승온도3\n",
    "        new_state.상승온도유지시간3 = self.상승온도유지시간3\n",
    "        return new_state\n",
    "    \n",
    "class QLearning:\n",
    "    def __init__(self, alpha=0.5, gamma=1, epsilon=0.1):\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.q_table = {}\n",
    "        self.rewards_table = {}\n",
    "        \n",
    "    def get_q_value(self, state, action):\n",
    "        state_str = str(state).replace(\" \", \"\")\n",
    "        action_str = str(action).replace(\" \", \"\")\n",
    "        state_action = state_str + \"_\" + action_str\n",
    "        if state_action not in self.q_table:\n",
    "            self.q_table[state_action] = 0\n",
    "            self.rewards_table[state_action] = 0 # rewards_table 업데이트\n",
    "        return self.q_table[state_action]\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        if np.random.uniform() < self.epsilon:\n",
    "            return Action()\n",
    "        else:\n",
    "            actions = [Action() for _ in range(10)]\n",
    "            q_values = [self.get_q_value(state, action) for action in actions]\n",
    "            max_q_value = max(q_values)\n",
    "            if q_values.count(max_q_value) > 1:\n",
    "                best_actions = [a for a in actions if self.get_q_value(state, a) == max_q_value]\n",
    "                return random.choice(best_actions)\n",
    "            else:\n",
    "                return actions[np.argmax(q_values)]\n",
    "\n",
    "    def learn(self, state, action, reward, next_state):\n",
    "        td_target = reward + self.gamma * max([self.get_q_value(next_state, Action()) for _ in range(10)])\n",
    "        td_error = td_target - self.get_q_value(state, action)\n",
    "        state_action = str(state).replace(\" \", \"\") + \"_\" + str(action).replace(\" \", \"\")\n",
    "        self.q_table[state_action] += self.alpha * td_error\n",
    "        self.rewards_table[state_action] = reward # 리워드 값을 저장\n",
    "\n",
    "    def get_reward(self, state_ks, new_state_ks):\n",
    "        if new_state_ks < state_ks:\n",
    "            return 1-new_state_ks\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "\n",
    "    \n",
    "    def print_q_table(self):\n",
    "        print(self.q_table)\n",
    "        \n",
    "q_learning  = QLearning()\n",
    "\n",
    "class QValue:\n",
    "    def __init__(self, n_episodes, n_iterations, state_action_attrs):\n",
    "        self.n_episodes = n_episodes\n",
    "        self.n_iterations = n_iterations\n",
    "        self.state_action_attrs = state_action_attrs\n",
    "        self.best_state = None\n",
    "        self.best_ks_value = float('inf')\n",
    "        self.best_e_value = float('inf')\n",
    "        self.best_iteration = None\n",
    "        self.episode_data = []\n",
    "        self.column_names = ['Episode', '상승속도1', '상승속도2', '상승속도3', '상승온도3', '상승온도유지시간3', 'Best_KS_Value','Reward','Iteration', 'New_KS_state', 'KS_Value', 'e_Value']\n",
    "        \n",
    "        \n",
    "    def run(self, q_learning):\n",
    "        for episode in range(self.n_episodes):\n",
    "            state = State()\n",
    "            for iteration in range(self.n_iterations):\n",
    "                action = q_learning.choose_action(state)\n",
    "                new_state = action.apply(state)\n",
    "                reward = q_learning.get_reward(state.get_ks(),new_state.get_ks())\n",
    "                q_learning.learn(state, action, reward, new_state)\n",
    "                state = new_state\n",
    "\n",
    "                if new_state.get_ks() < self.best_ks_value :\n",
    "                    self.best_ks_value = new_state.get_ks()\n",
    "                    self.best_state = new_state\n",
    "                    self.best_iteration = iteration + 1\n",
    "                    self.best_reward = reward\n",
    "                \n",
    "\n",
    "            \n",
    "            episode_data = {\n",
    "                    'Episode': episode+1,\n",
    "                    '상승속도1': self.best_state.상승속도1,\n",
    "                    '상승속도2': self.best_state.상승속도2,\n",
    "                    '상승속도3': self.best_state.상승속도3,\n",
    "                    '상승온도3': self.best_state.상승온도3,\n",
    "                    '상승온도유지시간3': self.best_state.상승온도유지시간3,\n",
    "                    'Best_KS_Value': self.best_ks_value[0],\n",
    "                'Reward': self.best_reward,\n",
    "                'Iteration': iteration+1,\n",
    "                'New_KS_state': str(new_state), \n",
    "                'KS_Value': new_state.get_ks()}\n",
    "            self.episode_data.append(episode_data)\n",
    "\n",
    "\n",
    "            \n",
    "        return pd.DataFrame(self.episode_data, columns=self.column_names)\n",
    "    \n",
    "def get_user_input():\n",
    "    attrs = ['Dor_Red', 'Dor_Blue', 'Dor_BK', 'Dor_DarkGrey', 'Dor_Brown', 'Dia_Yellow', 'Dia_Red', 'Dia_Blue', 'Dia_Black', 'Dia_Grey', 'Dia_YellowBrown', 'Dor_Yellow', 'Dor_Black']\n",
    "    user_input_dict = {}\n",
    "\n",
    "    for attr in attrs:\n",
    "        user_input = input(f\"{attr} 값을 입력하세요 (비어있으면 0으로 처리): \")\n",
    "        if user_input == '':\n",
    "            user_input = 0\n",
    "        else:\n",
    "            user_input = float(user_input)\n",
    "        user_input_dict[attr] = user_input\n",
    "    global sum_values  # 전역 변수를 수정하려면 global 키워드를 사용해야 합니다.\n",
    "    sum_values = sum(list(user_input_dict.values())) \n",
    "    user_input_dict['배합_Sunsolt'] = 0.3\n",
    "    user_input_dict['배합_빙초산'] = 0.2\n",
    "    \n",
    "    return user_input_dict, sum_values  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "678cfd88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최적의 공정을 알고 싶은가요?네\n",
      "Dor_Red 값을 입력하세요 (비어있으면 0으로 처리): 1\n",
      "Dor_Blue 값을 입력하세요 (비어있으면 0으로 처리): \n",
      "Dor_BK 값을 입력하세요 (비어있으면 0으로 처리): \n",
      "Dor_DarkGrey 값을 입력하세요 (비어있으면 0으로 처리): \n",
      "Dor_Brown 값을 입력하세요 (비어있으면 0으로 처리): \n",
      "Dia_Yellow 값을 입력하세요 (비어있으면 0으로 처리): \n",
      "Dia_Red 값을 입력하세요 (비어있으면 0으로 처리): \n",
      "Dia_Blue 값을 입력하세요 (비어있으면 0으로 처리): \n",
      "Dia_Black 값을 입력하세요 (비어있으면 0으로 처리): \n",
      "Dia_Grey 값을 입력하세요 (비어있으면 0으로 처리): \n",
      "Dia_YellowBrown 값을 입력하세요 (비어있으면 0으로 처리): \n",
      "Dor_Yellow 값을 입력하세요 (비어있으면 0으로 처리): \n",
      "Dor_Black 값을 입력하세요 (비어있으면 0으로 처리): \n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (10,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcess:\u001b[39m\u001b[38;5;124m\"\u001b[39m, q_value\u001b[38;5;241m.\u001b[39mbest_state)\n\u001b[0;32m     15\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest KS:\u001b[39m\u001b[38;5;124m\"\u001b[39m, q_value\u001b[38;5;241m.\u001b[39mbest_ks_value[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m---> 17\u001b[0m main()\n",
      "Cell \u001b[1;32mIn[4], line 12\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m q_learning \u001b[38;5;241m=\u001b[39m QLearning(alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, gamma\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.9\u001b[39m, epsilon\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m)\n\u001b[0;32m     11\u001b[0m q_value \u001b[38;5;241m=\u001b[39m QValue(n_episodes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, n_iterations\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, state_action_attrs\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m상승속도1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m상승속도2\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m상승속도3\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m상승온도3\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m상승온도유지시간3\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m---> 12\u001b[0m q \u001b[38;5;241m=\u001b[39m q_value\u001b[38;5;241m.\u001b[39mrun(q_learning)  \u001b[38;5;66;03m# sum_values를 전달\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDye&Chemical:\u001b[39m\u001b[38;5;124m\"\u001b[39m, user_input_values)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcess:\u001b[39m\u001b[38;5;124m\"\u001b[39m, q_value\u001b[38;5;241m.\u001b[39mbest_state)\n",
      "Cell \u001b[1;32mIn[3], line 178\u001b[0m, in \u001b[0;36mQValue.run\u001b[1;34m(self, q_learning)\u001b[0m\n\u001b[0;32m    176\u001b[0m state \u001b[38;5;241m=\u001b[39m State()\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m iteration \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_iterations):\n\u001b[1;32m--> 178\u001b[0m     action \u001b[38;5;241m=\u001b[39m q_learning\u001b[38;5;241m.\u001b[39mchoose_action(state)\n\u001b[0;32m    179\u001b[0m     new_state \u001b[38;5;241m=\u001b[39m action\u001b[38;5;241m.\u001b[39mapply(state)\n\u001b[0;32m    180\u001b[0m     reward \u001b[38;5;241m=\u001b[39m q_learning\u001b[38;5;241m.\u001b[39mget_reward(state\u001b[38;5;241m.\u001b[39mget_ks(),new_state\u001b[38;5;241m.\u001b[39mget_ks())\n",
      "Cell \u001b[1;32mIn[3], line 139\u001b[0m, in \u001b[0;36mQLearning.choose_action\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m    137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m random\u001b[38;5;241m.\u001b[39mchoice(best_actions)\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 139\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m actions[np\u001b[38;5;241m.\u001b[39margmax(q_values)]\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36margmax\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:1242\u001b[0m, in \u001b[0;36margmax\u001b[1;34m(a, axis, out, keepdims)\u001b[0m\n\u001b[0;32m   1155\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1156\u001b[0m \u001b[38;5;124;03mReturns the indices of the maximum values along an axis.\u001b[39;00m\n\u001b[0;32m   1157\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1239\u001b[0m \u001b[38;5;124;03m(2, 1, 4)\u001b[39;00m\n\u001b[0;32m   1240\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1241\u001b[0m kwds \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkeepdims\u001b[39m\u001b[38;5;124m'\u001b[39m: keepdims} \u001b[38;5;28;01mif\u001b[39;00m keepdims \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39m_NoValue \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[1;32m-> 1242\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _wrapfunc(a, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124margmax\u001b[39m\u001b[38;5;124m'\u001b[39m, axis\u001b[38;5;241m=\u001b[39maxis, out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:54\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     52\u001b[0m bound \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(obj, method, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m bound \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m bound(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:43\u001b[0m, in \u001b[0;36m_wrapit\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n\u001b[0;32m     42\u001b[0m     wrap \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m---> 43\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(asarray(obj), method)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wrap:\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, mu\u001b[38;5;241m.\u001b[39mndarray):\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (10,) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    start_command = input(\"최적의 공정을 알고 싶은가요?\")\n",
    "    if start_command.strip().lower() == '네':\n",
    "        user_input_values, _ = get_user_input()  # 반환된 값을 변수에 할당\n",
    "\n",
    "        # 동적 변수 이름을 사용하여 값을 할당합니다.\n",
    "        for attr, value in user_input_values.items():\n",
    "            globals()[attr] = value\n",
    "\n",
    "        q_learning = QLearning(alpha=0.2, gamma=0.9, epsilon=0.5)\n",
    "        q_value = QValue(n_episodes=100, n_iterations=100, state_action_attrs=['상승속도1', '상승속도2', '상승속도3', '상승온도3', '상승온도유지시간3'])\n",
    "        q = q_value.run(q_learning)  # sum_values를 전달\n",
    "        print(\"Dye&Chemical:\", user_input_values)\n",
    "        print(\"Process:\", q_value.best_state)\n",
    "        print(\"Best KS:\", q_value.best_ks_value[0])\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a5aec9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
